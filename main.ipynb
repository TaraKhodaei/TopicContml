{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd1a65b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265b6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# to generate dictionary of unique tokens\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#LDA model\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "\n",
    "# to suppress warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "#plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#To extract my sequences from myfile.phy\n",
    "import phylip\n",
    "\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "merging= True\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    #To print progress of the training procedure on screen\n",
    "    import logging\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.NOTSET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95cb5c",
   "metadata": {},
   "source": [
    "## Tokenize Documents\n",
    "\n",
    "Tokenization: Split the text into sentences and the sentences into words.\n",
    "\n",
    "Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ccaf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seq_list, k, gap_type='None', kmer='overlap'):    \n",
    "   \n",
    "    if gap_type == 'seq':\n",
    "        seq_list = [seq.replace('-', '') for seq in seq_list]\n",
    "        \n",
    "    elif gap_type == 'col':\n",
    "        # String List to Column Character Matrix Using zip() + map()\n",
    "        seq_matrix = np.transpose(list(map(list, zip(*seq_list))))      \n",
    "        #remove columns if the column has an element '-'\n",
    "        seq_matrix_cleaned = np.array([col for col in seq_matrix.T if '-' not in col]).T\n",
    "        #Convert List of lists to list of Strings again\n",
    "        seq_list = [''.join(ele) for ele in seq_matrix_cleaned] \n",
    "        \n",
    "    if DEBUG:\n",
    "        np.savetxt ('seq_list', seq_list,  fmt='%s')     \n",
    "    \n",
    "    #docs: list of lists of each document kemers\n",
    "    docs = []  \n",
    "    if kmer == 'overlap':\n",
    "        for seq in seq_list:       \n",
    "            doc=[]     \n",
    "            for i in range(len(seq) - k + 1):\n",
    "                kmer = seq[i:i+k]\n",
    "                doc.append(kmer)\n",
    "            docs.append(doc) \n",
    "            \n",
    "    elif kmer == 'not_overlap':\n",
    "        for seq in seq_list:   \n",
    "            doc = [seq[i:i+k] for i in range(0, len(seq), k)]\n",
    "            docs.append(doc) \n",
    "    return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585b442b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tara/Desktop/TPContml_project\n",
      "\n",
      "============= locus 0 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 33360\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 33360\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(581 unique tokens: ['A', 'AA', 'AAAA', 'AAAAAC', 'AAAATGAT']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(300 unique tokens: ['A', 'AAACCCTC', 'AACTAA', 'AACTAATG', 'AACTAT']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 300\n",
      "\n",
      "============= locus 1 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 26064\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 26064\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(370 unique tokens: ['??', '????', '?????TTA', '?T', '?TTA']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(91 unique tokens: ['AAAAGT', 'AAAAGTAT', 'AAAT', 'AAATAC', 'AAATACAC']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 91\n",
      "\n",
      "============= locus 2 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 21024\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 21024\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(255 unique tokens: ['AA', 'AAAC', 'AAACAG', 'AAACATAT', 'AAACCA']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(24 unique tokens: ['AGCGAA', 'CGAATCTG', 'GACTGG', 'GACTGGGG', 'GTGT']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 24\n",
      "\n",
      "============= locus 3 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 12000\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 12000\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(180 unique tokens: ['AA', 'AAAC', 'AAACCG', 'AAAG', 'AAAGCTGG']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(19 unique tokens: ['ATCC', 'ATCCGAAT', 'ATGATG', 'CAATCC', 'GATG']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 19\n",
      "\n",
      "============= locus 4 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 12840\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 12840\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(269 unique tokens: ['A', 'AA', 'AAAC', 'AAAG', 'AAAGAA']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(13 unique tokens: ['AATA', 'AGTATG', 'ATGCTGCT', 'CAGGAT', 'GTAC']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 13\n",
      "\n",
      "============= locus 5 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 30696\n",
      "merging_nums = [18, 8, 8, 2, 4, 4]\n",
      "Test ===> Number of all words in docs_merged = 30696\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(1037 unique tokens: ['??', '????CT', '??CT', '??CTTTTT', 'AA']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(437 unique tokens: ['??CT', 'AAATTT', 'AATA', 'AATTGGGC', 'ACTGCCCC']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 437\n",
      "\n",
      "============= locus 6 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 39640\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 39640\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(801 unique tokens: ['??', '????', 'AA', 'AAAA', 'AAAAAA']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(432 unique tokens: ['????', 'AAAAAATA', 'AAAACATA', 'AAAACCTC', 'AAAAGA']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 432\n",
      "\n",
      "============= locus 7 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 14172\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 14172\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(234 unique tokens: ['AA', 'AAAC', 'AAAG', 'AAAGAG', 'AAAGTC']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(47 unique tokens: ['AAGGACTT', 'ACAA', 'AGTT', 'AGTTTT', 'AGTTTTCA']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 47\n",
      "\n",
      "============= locus 8 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 11038\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 11038\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(184 unique tokens: ['AA', 'AACA', 'AACAGAGA', 'AAGA', 'AATATC']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(20 unique tokens: ['AACA', 'AACAGAGA', 'ACAC', 'ACACAC', 'ACACACAA']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 20\n",
      "\n",
      "============= locus 9 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 12582\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 12582\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(258 unique tokens: ['AA', 'AACCAT', 'AC', 'ACAC', 'ACACAG']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(94 unique tokens: ['AGTTCA', 'AGTTCATC', 'ATCCGT', 'ATGGAGAA', 'CAAC']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 94\n",
      "\n",
      "============= locus 10 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 12768\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 12768\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(272 unique tokens: ['AA', 'AAAG', 'AAAT', 'AAATTG', 'AACC']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(219 unique tokens: ['AACCCA', 'AACT', 'AACTCACA', 'AAGG', 'AAGTTC']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 219\n",
      "\n",
      "============= locus 11 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 9792\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 9792\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(127 unique tokens: ['AA', 'AAAATT', 'AAAG', 'AAAGTC', 'AAGCCT']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(7 unique tokens: ['CAATGA', 'CAATGAAT', 'GTTTTTTT', 'TTTT', 'TTTTTT']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 7\n",
      "\n",
      "============= locus 12 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 20548\n",
      "merging_nums = [16, 6, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 20548\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(264 unique tokens: ['A', 'AA', 'AAA', 'AAAATC', 'AAAC']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(8 unique tokens: ['TATCTA', 'TATCTACT', 'CTTG', 'CTTGCA', 'CTTGCAAG']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 8\n",
      "\n",
      "============= locus 13 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 41424\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 41424\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(517 unique tokens: ['AA', 'AAAA', 'AAAAAAAG', 'AAAAAC', 'AAAAAG']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(28 unique tokens: ['AAAAAAAG', 'AAAAAG', 'ACTGGG', 'ATGCTA', 'CCAT']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 28\n",
      "\n",
      "============= locus 14 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 36336\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 36336\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(374 unique tokens: ['AA', 'AAAA', 'AAAAAC', 'AAAAATTA', 'AAAACT']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(16 unique tokens: ['AACTTC', 'AACTTCAA', 'AAGGAAGT', 'AAGT', 'ACAG']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 16\n",
      "\n",
      "============= locus 15 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 26058\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 26058\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(540 unique tokens: ['AA', 'AAAA', 'AAAAAA', 'AAAAAAAA', 'AAAACAAC']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(318 unique tokens: ['AAAAAAAA', 'AAAACAAC', 'AAAATT', 'AAACTCTA', 'AAATGG']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 318\n",
      "\n",
      "============= locus 16 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 13320\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 13320\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(263 unique tokens: ['AA', 'AAAA', 'AAAACATG', 'AAAT', 'AACATG']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(65 unique tokens: ['AAAT', 'CATC', 'GCAA', 'AAATTCCC', 'ACCA']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 65\n",
      "\n",
      "============= locus 17 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 13776\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 13776\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(235 unique tokens: ['AA', 'AAAA', 'AAAATC', 'AAAC', 'AAACATAG']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(11 unique tokens: ['AATAATGC', 'ATGT', 'GCTC', 'GTTTGC', 'GTTTGCTC']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 11\n",
      "\n",
      "============= locus 18 =============\n",
      "\n",
      "Test ===> Number of all words in docs = 23568\n",
      "merging_nums = [18, 8, 10, 2, 6, 4]\n",
      "Test ===> Number of all words in docs_merged = 23568\n",
      "\n",
      "Dictionary:\n",
      "Dictionary(262 unique tokens: ['AA', 'AAAA', 'AAAACA', 'AAACCA', 'AAACTA']...)\n",
      "\n",
      "Dictionary after filtering:\n",
      "Dictionary(12 unique tokens: ['AAACTA', 'ACTATACA', 'CATCTCCC', 'CCATCA', 'TCCC']...)\n",
      "\n",
      "Number of documents: 6\n",
      "Number of unique tokens: 12\n"
     ]
    }
   ],
   "source": [
    "current = os.getcwd()\n",
    "print(current) \n",
    "\n",
    "topics_loci=[]\n",
    "num_loci=19\n",
    "\n",
    "for i in range(num_loci):\n",
    "    print(f'\\n============= locus {i} =============\\n')\n",
    "    \n",
    "    #================= Extract labels and sequences ==================\n",
    "    locus_i = 'locus'+str(i)+'.txt'\n",
    "    locus = os.path.join(current,'loci',locus_i)\n",
    "    \n",
    "    label,sequence,varsites = phylip.readData(locus, type='NotSTANDARD')\n",
    "    #print(f\"label = {label}\")\n",
    "    #print(f\"sequence = {sequence}\")    \n",
    "    \n",
    "    #========================= Extract k-mers ========================\n",
    "    docs=[]\n",
    "    for k in range(2,10,2):    \n",
    "#         tokenize_k = tokenize(sequence, k )       #\"with gap\" and \"overlapped k-mers\"\n",
    "        tokenize_k = tokenize(sequence, k, gap_type='seq', kmer = 'not_overlap')     #\"No gaps'-'\" in each sequence and \"No overlapped k-mers\"\n",
    "#         tokenize_k = tokenize(sequence, k, kmer = 'not_overlap' )    #\"with gap\" and \"No overlapped k-mers\"\n",
    "        \n",
    "        if len(docs)>0:\n",
    "            docs =[docs[i]+tokenize_k[i] for i in range(len(tokenize_k))]\n",
    "        else:\n",
    "            docs = tokenize_k  \n",
    "    #count number of all words in docs\n",
    "    count = 0\n",
    "    for doc in docs:\n",
    "        count += len(doc)   \n",
    "    print(f\"Test ===> Number of all words in docs = {count}\")\n",
    "    \n",
    "    \n",
    "    #====================== k-mers after merging =====================\n",
    "    if merging:\n",
    "        letters = ['sca','sce','sct','smc', 'smi', 'sms']\n",
    "        merging_nums=[]\n",
    "        for item in letters:\n",
    "            merg_indxs= [label.index(i) for i in label if item in i]\n",
    "            merging_nums.append(merg_indxs)\n",
    "            \n",
    "        merging_nums=[len(i) for i in merging_nums]\n",
    "        print(f\"merging_nums = {merging_nums}\")\n",
    "        \n",
    "        docs_merged=[]\n",
    "        j=0\n",
    "        for num in merging_nums:\n",
    "            doc_merged = list(itertools.chain.from_iterable(docs[j:j+num])) \n",
    "            docs_merged.append(doc_merged)\n",
    "            j +=num\n",
    "\n",
    "        #count number of all words in docs\n",
    "        count = 0\n",
    "        for doc in docs_merged:\n",
    "            count += len(doc)   \n",
    "        print(f\"Test ===> Number of all words in docs_merged = {count}\") \n",
    "\n",
    "    #=================== Dictionary of Unique Tokens ==================\n",
    "    docs = docs_merged\n",
    "    dictionary = Dictionary(docs)\n",
    "    print(f\"\\nDictionary:\\n{dictionary}\")\n",
    "    \n",
    "    #============ Filtering: remove rare and common tokens ============\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "    print(f\"\\nDictionary after filtering:\\n{dictionary}\")\n",
    "    \n",
    "    #================== Vectorize data: Bag-of-words ==================\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    print(f'\\nNumber of documents: {len(corpus)}')\n",
    "    print(f'Number of unique tokens: {len(dictionary)}')\n",
    "    \n",
    "    #======================= LDA Model: Training ======================\n",
    "    # Set training parameters.\n",
    "    num_topics = 5    \n",
    "    chunksize = 20    \n",
    "    passes = 50     \n",
    "    iterations = 1000  \n",
    "    eval_every = 1\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = dictionary[0]  \n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha=1, eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every, minimum_probability=0, update_every=5 )\n",
    "\n",
    "    #================== Print topics/words frequencies ================\n",
    "    if DEBUG:\n",
    "        for idx, topic in model.print_topics(-1):\n",
    "            print(\"Topic: {} \\nWords: {}\\n\".format(idx, topic))\n",
    "            \n",
    "    #=============== Assigning the topics to the documents ============\n",
    "    docs_tuples = model[corpus]\n",
    "\n",
    "#     for num, doc_i in enumerate(docs_tuples):\n",
    "#         print(f\"doc {num}     {doc_i}\")\n",
    "            \n",
    "    #==================== topics list for current locus ===============            \n",
    "    delta_type='dirichlet'\n",
    "    topics = []\n",
    "    for num, doc in enumerate(docs_tuples):\n",
    "        first=[]\n",
    "        second=[]\n",
    "        for tuple_i in doc:\n",
    "            first.append(tuple_i[0])\n",
    "            second.append(tuple_i[1])\n",
    "        \n",
    "        if delta_type == 'equal':\n",
    "            sum_frequencies = sum(second)\n",
    "            res = 1- sum_frequencies\n",
    "            delta = (np.ones(num_topics-len(doc)))*(res/(num_topics-len(doc)))\n",
    "            \n",
    "        elif delta_type == 'dirichlet':\n",
    "            sum_frequencies = sum(second)\n",
    "            res = 1- sum_frequencies\n",
    "            delta = ((np.random.dirichlet(np.ones(num_topics-len(doc)),size=1))*res).flatten()\n",
    "\n",
    "        topics_freq=[]\n",
    "        j=0\n",
    "        for i in range(num_topics):\n",
    "            if i in first:\n",
    "                topics_freq.append(second[first.index(i)])\n",
    "            else:\n",
    "                topics_freq.append(delta[j])\n",
    "                j +=1\n",
    "        topics.append(topics_freq)\n",
    "        \n",
    "    #print(f'\\nTopics :\\n{topics}')\n",
    "    topics_loci.append(topics) \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930c2588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Write results in \"infile\" to use in CONTML ======= \n",
      "\n",
      "\u001b[2J\u001b[H\n",
      "\u001b[2J\u001b[H\n",
      "Continuous character Maximum Likelihood method version 3.697\n",
      "\n",
      "Settings for this run:\n",
      "  U                       Search for best tree?  Yes\n",
      "  C  Gene frequencies or continuous characters?  Gene frequencies\n",
      "  A   Input file has all alleles at each locus?  No, one allele missing at each\n",
      "  O                              Outgroup root?  No, use as outgroup species 1\n",
      "  G                      Global rearrangements?  No\n",
      "  J           Randomize input order of species?  No. Use input order\n",
      "  M                 Analyze multiple data sets?  No\n",
      "  0         Terminal type (IBM PC, ANSI, none)?  ANSI\n",
      "  1          Print out the data at start of run  No\n",
      "  2        Print indications of progress of run  Yes\n",
      "  3                              Print out tree  Yes\n",
      "  4             Write out trees onto tree file?  Yes\n",
      "\n",
      "  Y to accept these or type the letter for one to change\n",
      "Adding species:\n",
      "   1. sca       \n",
      "   2. sce       \n",
      "   3. sct       \n",
      "   4. smc       \n",
      "   5. smi       \n",
      "   6. sms       \n",
      "\n",
      "Output written to file \"outfile\"\n",
      "\n",
      "Tree also written onto file \"outtree\"\n",
      "\n",
      "Done.\n",
      "\n",
      "(smc:0.00822700,(smi:0.05708344,(sms:0.02529409,(sce:0.01073619,sct:0.03681491):0.29030045):0.02682581):0.00000000, sca:0.33470303); \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: outfile: No such file or directory\n",
      "rm: outtree: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javax.swing.UIManager$LookAndFeelInfo[Metal javax.swing.plaf.metal.MetalLookAndFeel]\n",
      "javax.swing.UIManager$LookAndFeelInfo[Nimbus javax.swing.plaf.nimbus.NimbusLookAndFeel]\n",
      "javax.swing.UIManager$LookAndFeelInfo[CDE/Motif com.sun.java.swing.plaf.motif.MotifLookAndFeel]\n",
      "javax.swing.UIManager$LookAndFeelInfo[Mac OS X com.apple.laf.AquaLookAndFeel]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the fonts \"Times\" and \"Times\" are not available for the Java logical font \"Serif\", which may have unexpected appearance or behavior. Re-enable the \"Times\" font to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = ['sca','sce','sct','smc', 'smi', 'sms']\n",
    "\n",
    "def infile(topics_loci_concatenated, lletters):     \n",
    "    num_pop= len(topics_loci_concatenated)\n",
    "    with  open(\"infile\", \"w\") as f:   \n",
    "        f.write('     {}    {}\\n'.format(num_pop, num_loci)) \n",
    "        f.write('{} '.format(num_topics)*num_loci) \n",
    "        f.write('\\n') \n",
    "        for i in range(num_pop):\n",
    "            #myname=\"L\"+str(i)\n",
    "            myname = lletters[i]\n",
    "            f.write(f'{myname}{\" \"*(11-len(myname))}{\" \".join(map(str, topics_loci_concatenated[i]))}\\n')            \n",
    "    f.close()\n",
    "\n",
    "        \n",
    "\n",
    "def run_contml(infile): \n",
    "    os.system('rm outfile outtree')\n",
    "    os.system('echo \"y\" | /Users/tara/bin/contml') \n",
    "    \n",
    "    #read the outtree file\n",
    "    with open('outtree', 'r') as f:\n",
    "        tree = f.read().replace('\\n', ' ')\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'\\n========== Write results in \"infile\" to use in CONTML ======= \\n')\n",
    "\n",
    "#for each locus remove last column from topic matrix\n",
    "topics_loci_missingLast = [[item[i][:-1] for i in range(len(item))] for item in topics_loci]\n",
    "\n",
    "\n",
    "#concatenation of the topics for al loci\n",
    "topics_loci_concatenated = topics_loci_missingLast[0]\n",
    "for i in range(1,len(topics_loci_missingLast)):\n",
    "    topics_loci_concatenated = [a+b for a, b in zip(topics_loci_concatenated, topics_loci_missingLast[i]) ]\n",
    "    \n",
    "#print(f'topics_loci_concatenated =\\n{topics_loci_concatenated}')\n",
    "\n",
    "\n",
    "\n",
    "#generate infile\n",
    "infile(topics_loci_concatenated, letters)\n",
    "\n",
    "\n",
    "#run CONTML\n",
    "ourtree = run_contml(infile)\n",
    "print(ourtree)\n",
    "\n",
    "\n",
    "#Figtree\n",
    "os.system(\"/Users/tara/bin/figtree outtree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
